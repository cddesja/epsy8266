---
title: "CFA of WISC-R"
author: "Christopher Desjardins"
date: "March 5, 2019"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Background on the data
The WISC-R is a revised version of the WISC (a downward extension of the Weschler-Belleview test for children) published in 1975. Data are from an administration of the WISC-R to 175 children. Details about the data can be found in Tabacknick & Fidell, Using Multivariate Statistics, 3rd ed.
Variables in the data include `client` (this is the id variable), `agemat` (an age categorical variable), and 10 core subtest and 1 optional subtest that measure verbal and performance IQ.

### Reading in data and exploring the data

The first thing that we need to do is to read the data into `R`. The `wiscr` data set is in SPSS format and is stored in the data file called `wiscsem.sav`. To read in this data set we call the `read.spss`  function from the foreign package and then view the first few rows of data. 

```{r}
wiscr <- foreign::read.spss("wiscsem.sav", to.data.frame = T)
head(wiscr)
```

It is always good idea to view a summary of our data to ensure that it has been read in correctly. 

```{r}
summary(wiscr)
apply(wiscr, 2, var)
```

There are no strange `Min.` or `Max.` values for any of the variables and it looks like there are no missing data (i.e, there is nothing that says `NA's`). Are these variables on the same scale?

Since we want to do a confirmatory factor analysis (CFA) with this data it's a good idea to look at histograms, box plots, stem and leaf plots, and bivariate relationships between the manifest variables using a scatter plot to get a sense of potential influential/outlying observations. To make a scatter plot matrix we can use the `pairs()` function. Since we have 11 variables, we would have 66 unique subgraphs! So, we'll look at the scatter plot matrix by the latent construct they are theoretically suppose to be manifestations of. 

We'll start by making box plots.
```{r, fig.height=7}
wiscr.sub <- wiscr[,-(1:2)]
par(mfrow = c(3, 4),
    mar = c(1, 2, 2, 1) + .1)
for(i in 1:11){
  boxplot(wiscr.sub[,i], main = paste(names(wiscr.sub))[i])
}
```

Histograms can also be created by:

```{r, fig.height=7}
par(mfrow = c(3, 4),
    mar = c(1.5, 2, 2, 1) + .1)
for(i in 1:11){
  hist(wiscr.sub[,i], main = paste(names(wiscr.sub))[i])
}
```

For most variables, there are a few observations that are outliers. These outliers could be problematic moving forward and could influence our findings and therefore the stability of our estimated parameters. However, what is a univariate outlier or influential point may not be a multivariate outlier or influential point.  Some of these variables are also quite skewed as indicated by the asymmetry of the boxes. This is less clear in the histograms. We could also make stem-and-leaf plots, too, which I think are very helpful for examining normality.

```{r}
for(i in 1:11){
  cat("The variable is ",names(wiscr.sub)[i], ".\n", sep = "")
  stem(wiscr.sub[,i])
}
```

Note, that regular maximum likelihood estimation requires multivariate normality and multivariate normality implies univariate normality BUT univariate normality doesn't imply multivariate normality. We could use a robust estimator if we thought we had strong violations. It's not clear to me that we do.

Next lets look at the bivariate scatter plots starting with the manifest variables for the verbal factor.

```{r}
# this function will write the correlations in the upper diagonal
# just ignore the details of this function. I modified it from
# ?pairs
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- cor(x, y)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

dev.off() # this resets par()
verb.scat <- subset(wiscr.sub, select = c("info", "comp", "arith", "simil", "digit", "vocab"))
pairs(verb.scat, upper.panel = panel.cor)
```

In general, we see moderately strong, positive relationships between all the subtests with the possible exception of `digit`, which appears to be less strongly correlated with the other subtests. There are some values that look like they are outlying, too. 

Next we examine the manifest variables for the performance factor.

```{r}
perf.scat <- subset(wiscr.sub, select = !(names(wiscr.sub) %in% names(verb.scat)))
pairs(perf.scat, upper.panel = panel.cor)
```

Again, we see moderately positive relationships between all the subtests with the exception of `coding` and some potential outlying observations.

To investigate influence, we can also look at Mahalanobis distance using the `faoutlier` package. This measures the distance an observation is from the centriod (a multidimensional mean).

```{r, message = FALSE}
# Install devtools
# install.packages('devtools')

# Install faoutlier from GitHub
# devtools::install_github('philchalmers/faoutlier')

wiscr.mah <- faoutlier::robustMD(wiscr)
plot(wiscr.mah)
which.max(wiscr.mah$mah)
```

There are a few observations near 40 that look influential. A sensible thing to do would be to consider the robustness of our CFA findings and/or your covariance/correlation matrix to omitting these observations (i.e., as a sensitivity analysis). I do this below

```{r}
which(wiscr.mah$mah > 40) # identify values
wiscr.nomah <- wiscr[-c(66, 170), ] # drop them
round(cor(wiscr.nomah) - cor(wiscr), 2) # the difference in the correlation matrices
```

What do you think we're looking for here? Where is the largest effect of dropping these two participants?

I leave that as an exercise for you for the CFA. If you had values that were very far from the pack and a small sample size, then it would be even more imperative to do this. However, later we look at Cook's distance and those are values that are influential based on your actual model and might be more important to investigate. 

For now, let's move on to performing the actual CFA.

### CFA using `lavaan`

To perform the actual confirmatory factor analysis in R we'll use `lavaan` package and we'll use the `semPlot` package to create path diagrams feeding the function from our `lavaan` objects.

```{r, message = FALSE}
# Install the package, if you haven't yet.
# install.packages("semPlot")

# Load the lavaan package
library("lavaan")
```

As a reminder, in `lavaan`, the following syntax is used

  + "=~" means is manifested by. This is how you define your latent variable.
  + "~~" is how you define a covariance or variance.
  + "~" is how you define a path for path analysis or SEM.
  + "+" is how you string together variables (like with `lm()`)

Since we know which subtests were suppose to measure which component of intelligence (i.e., cerbal or pwerformance), we will define our model to reflect this. But first, you'll note that Kline says to fit a single factor model and show it doesn't fit well. We'll do that here, but it's not clear to me how wide spread this practice is. Why would fit a model you know is wrong? Just for discriminant validity evidence, I guess ...

```{r}
one.mod <- '
  iq =~ info + comp + arith + simil + vocab + digit + pictcomp + parang + block + object + coding
'
one.fit <- cfa(mod = one.mod, data = wiscr)
fitmeasures(one.fit, c("chisq", "df", "pvalue", "rmsea", "tli", "cfi", "srmr"))
```

I have printed a few ways to evaluate model fit. First, is the chi-square test (with it's df and p-value). It is a badness-of-fit test statistic. It tests whether a fully-saturated model (i.e., a model that would perfectly predict the covariance matrix of our sample) fits better than our specified, more restrictive model.

H0: No difference in model fit
H1: More complex (fully saturated model) fits better

This p < .05, which means are lrestricted model doesn't fit as well as the unrestricted model. The remaining fit measures indicate poor fit as well. Let's more on to the more appropriate and test whether it's an improvement in model fit using a chi-square test of difference. 

```{r}
two.mod <- '
  verb =~ info + comp + arith + simil + vocab + digit
  perf =~ pictcomp + parang + block + object + coding
'
two.fit <- cfa(mod = two.mod, data = wiscr)
anova(one.fit, two.fit)
```

From the output, we see that we reject the chi-square test of difference, which states that the less restricted model (the two factor model) is an improvement in fit over the more restricted model (one factor model). This is discriminant validity evidence.

Before we look at the output, let's see what a path diagram looks like for this model.

```{r}
semPlot::semPaths(two.fit)
```

This diagram shows all the paths being estimated. Why is the path to each indicator "dashed"?

Let's also inspect the model to verify that `lavaan` actually fit the model we anticipated.

```{r}
inspect(two.fit)
```

In the `$lamba$` matrix, we see which factor loadings (pattern coefficients) are being estimated. This is indicated by non-zero values. So, you'll note that `info` and `pictcomp` are both zero. This is so that the model can be identified (i.e., we are using these to define a scale for our factor). What kind of identification is this called?

Next, is `$theta`, the variance-covariance matrix of our manifest variables, this refers to the residual/error variances for our manifest variables, i.e., the unique or specific variance. This is variability that is unexplained by our factor models. You'll see that all the manifest variables have this estimated and we have no covariances estimated. This means that residuals are uncorrelated (i.e., locally-independent) and we have simple structure.

Finally, you can `$psi` which is the variance-covariance matrix of our factors. You'll note that the elements of this matrix (the variances and covariance) are being estimated (i.e., we are assuming verbal and performance intelligence are correlated). Unlike LISREL notation, regardless of whether a latent variable is exogenous or endogenous latent variables (i.e., it will include disturbances) they will be in the $psi matrix. In LISREL notation, psi contains only endogenous latent variables disturbances and the phi matrix contains exogenous latent variables variances/covariances.

Can you write this model in LISREL notation?

To obtain the default summary output, we'll call the summary function over our `fit` object.

```{r}
summary(two.fit)
```

The output is meant to be similar to `Mplus`. However, the default output is a bit more terse. The "Minimum Function Test Statistic", is the chi-square statistic I described above. Because we reject the null hypothesis this means that our restricted model doesn't fit as well as the fully-saturated model and suggests a poor fitting model. 

After this we see the parameter estimates. The major thing to note here is that `coding` doesn't appear to be related to performance IQ (based on the z-tests).

To obtain the standardized solution and get the fit indices in the output:

```{r}
summary(two.fit, standardized = TRUE, fit.measures = TRUE)
```

Because all of our latent variables are continuous, we can use `Std.all` for all the variables (which standardizes both the observed and latent variables). The interpretation is the same as beta weights. For example, for info, 0.423 means for a 1 standard deviation in verbal, the expected increase in info is 0.423 standard deviations. If we square these that is the proportion of the manifest variable explained by the factor (i.e r-squared) because we have a simple solution.  If we had dichotomous predictor variables, we would use `Std.lv`, which represents the expected difference/change in standard deviations in the outcome variable as the dichotomous variable goes from 0 to 1. Note the `Std.lv` only standardized the latent variable.  For comp, we can interpret the unstandardized pattern coefficient as "we would expect a .926 unit increase in comp for a 1 unit increase the verbal factor". 

Large standardized pattern coefficients also suggest convergent validity. Kline's criteria is that the overwhelming majority of these should be greater than .70 (i.e, share 50% or more of their variance is explained by the factor). This is a reasonable assertion. Based on this criteria, would you say we have convergent validity?

The fit indices are printed at the top. We want CFI and TLI to be large preferably at least .95. This information is is followed by the log-likelihoods of our current model and the unrestricted model (a fully saturated model). Followed by the number of free parameters, we can get this from inspect too, and the AIC and BIC, which are calculated based on the log-likelihood and df. Finally, the RMSEA and SRMR are presented and we want these to be small. Preferably, smaller than .05 but less than .08 is at least okay. 

Please note, that some SEM methodologists don't think we should look at these statistics at all and instead should only interpret the chi-square test. If we reject this statistic, then we should stop. We'll talk more about fit a little later in the semester. If you want to request the standard SEM fit indices directly you can use the `fitmeasures` function that I showed earlier.

The output also shows a "Model test baseline model". What is this model? We talked about it earlier in the semester.

## Estimating intercepts

Unlike `Mplus`, `lavaan` does not estimate intercepts by default. If you are interested in these, you need to specify `meanstructure = TRUE` during the fitting routine. This will not change the overall fit of the model as `lavaan` will plug in the mean of your observed variables as the estimates (i.e., it doesn't change the degrees of freedom). Let's look at the model.

```{r}
fit.mean <- cfa(mod = two.mod, data = wiscr, meanstructure = TRUE)
inspect(fit.mean)
```

The big difference from inspect now is the presence of the `$nu`, which are the intercepts for the observed variables (i.e., their means) and `$alpha`, which are the means for verbal and performance. These are fixed to 0 by default. This can be confusing because while these parameters are being estimated, they are not really being estimated because we're just using the means for their estimates.  

## Residuals

We can and should inspect the residual covariance matrix to assess the local fit of our model. This will tell us which covariances/variances aren't being explained by our model. We will do this by examining the residual correlation matrix ($cov) and the standardized residuals ($cov.z). We are looking for correlations that are larger than .1 (in absolute value) and for the standardized residuals if a value is greater than 1.96 (in absolute value) this indicates a statistically significant lack of fit and they can be interpreted as z-tests.

```{r}
lavResiduals(two.fit)
```

Based on the residual correlation matrix, it looks like the covariance between pictcomp and comp is being overestimated by the model as are pictcomp and simil, parang and simil, and coding and vocab. The covariance between pictcomp and coding is being underestimated as is the covariance between object and arith. These values imply misfit for at least part of the covariance matrix.

Based on the standardized residuals, comp and info, comp and pictcomp, arith and object, simil and pictcomp, digit and coding, and pictcomp and coding are not being adequately explained by the model (z > 1.96). 

This mismatch can explain the poor fit seen above. More on this later!

## R-Squared

We can have lavaan give us the r-squared directly, which can also be calculated by just doing 1 - the standardized unique variance.

```{r}
summary(two.fit, rsquare = T)
inspect(two.fit, "rsquare")
```

The unstandardized error variance divided by the observed variance for the indicator is the proportion of variance not explained by the factor (or 1 - rsquared).

```{r}
obs.var <- apply(wiscr, 2, var)[3:13]
obs.var <- data.frame(obs.var = obs.var, lhs = names(obs.var))
par.ests <- parameterEstimates(two.fit)
error.var <- par.ests[par.ests$op == "~~", c("lhs", "est")]

# merge the data.frames
com.var <- merge(obs.var, error.var)

# proportion unexplained
com.var$prop.unq <- com.var$est / com.var$obs.var
com.var
```

## Model Test Baseline Model

Do you remember what the "Model test baseline model" was again?

```{r}
iq.baseline <- '
  info ~~ info
  comp ~~ comp
  arith ~~ arith
  simil ~~ simil
  digit ~~ digit
  vocab ~~ vocab
  pictcomp ~~ pictcomp
  parang ~~ parang
  block ~~ block
  object ~~ object
  coding ~~ coding
'
fit.base <- cfa(model = iq.baseline, data = wiscr)
summary(fit.base, fit.measures = T)
```

It's the model that estimates only variances and no other parameters. So the df of 55, is 66 (number of unique elements) - 11 (number of variances). This model is suppose to be the most parsimonious plausible model (a model with no association). But that's a pretty boring theoretical model of the world. 

## Model modifications

Returning to our model, Let's fit the parameter for coding to 0 in our the model and refit the model:

```{r}
mod.fixcoded <- '
  verb =~ info + comp + arith + simil + digit + vocab
  perf =~ pictcomp + parang + block + object + 0*coding
'
fit.fixcoded <- cfa(mod.fixcoded, wiscr)
summary(fit.fixcoded, fit.measures = TRUE, standardized = TRUE)
```

Let's compare the model with coding and without coding

```{r}
fixedcode.stats <- fitmeasures(fit.fixcoded, fit.measures = c("rmsea", "tli", "cfi"))
org.stats <- fitmeasures(two.fit, fit.measures = c("rmsea", "tli", "cfi"))   
rbind(fixedcode.stats, org.stats)
anova(fit.fixcoded, two.fit) # chi-square test of difference. 
```

We could consider dropping coding but dropping coding changes the intepretation of the construct. It is a more parsimonious model but if it's a theoretical important measure to our construct, it should probably remain. For now, let's drop coding, refit the model, and examine the modification indices.

```{r}
iq.cfa.nocode <- '
  verb =~ info + comp + arith + simil + digit + vocab
  perf =~ pictcomp + parang + block + object
'
fit.nocode <- cfa(iq.cfa.nocode, wiscr)
modindices(fit.nocode, sort. = TRUE)
```

Modification indices can be used to improve the fit of your model and look at the improvement in model fit (chi-square) if that path is freed. Our present model is nested within this.

The first column is the name of the path to free, followed by the expected change in chi-square, the expected parameter change (i.e., the estimated parameter) and then standardized versions of this: standardizing just the latent variable, standardizing all variables, standardizing all but exogenous variables in the model.  

The biggest change in model fit would be allowing comp to load onto perf. The change in chi-square should be 9.853.

```{r}
iq.cfa.emp <- '
  verb =~ info + comp + arith + simil + digit + vocab
  perf =~ pictcomp + parang + block + object + comp 
'

fit.emp <- cfa(iq.cfa.emp, wiscr)
summary(fit.emp, fit.measures = TRUE, standardized = TRUE)
```

Now we see that are fit measures are very good and that we fail to reject the null hypothesis for the chi-square test. However, does it make sense to load comp on perf? That's a validity question and has ramifications for generalizability as well. 

Now let's plot our best fitting model with the standardized parameters.

```{r}
semPlot::semPaths(fit.emp, what = "stand", fade = F)
```

Finally, let's see if there are any influential points using Cook's distance.

```{r}
cfa.cooks <- faoutlier::gCD(data = wiscr, mod = iq.cfa.emp)
plot(cfa.cooks)
```

There are definitely a few observations which are quite far from the rest. Let's see if the results change if we drop the value with the largest Cook's distance.

```{r}
which(cfa.cooks$gCD )
cfa.cooks$gCD[76]
fit.emp.no76 <- cfa(iq.cfa.emp, wiscr[-c(76),])
summary(fit.emp.no76)
```

What do you think? Did they change? What changed?
