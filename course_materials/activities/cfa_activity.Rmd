---
title: "CFA of WISC-R"
author: "Christopher Desjardins"
date: "March 4, 2019"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Background on the data
The WISC-R is a revised version of the WISC (a downward extension of the Weschler-Belleview test for children) published in 1975. Data are from an administration of the WISC-R to 175 children. Details about the data can be found in Tabacknick & Fidell, Using Multivariate Statistics, 3rd ed.
Variables in the data include `client` (this is the id variable), `agemat` (an age categorical variable), and 10 core subtest and 1 optional subtest that measures verbal and performance IQ.

### Reading in data and exploring the data

The first thing that we need to do is to read the data into `R`. The `wiscr` data set is in SPSS format and is stored in the data file called `wiscsem.sav`. To read in this data set we load the `foreign` library, define the path to the data directory (optional), call the `read.spss` function, and then view the first few rows of data. 

```{r}
library("foreign")
wiscr <- foreign::read.spss("wiscsem.sav", to.data.frame = T)
head(wiscr)
```

It is always good idea to view a summary of our data to ensure that it has been read in correctly. 

```{r}
summary(wiscr)
```

There are no strange `Min.` or `Max.` values for any of the variables and it looks like there are no missing data (i.e, there is nothing that says `NA's`). 

Since we want to do a confirmatory factor analysis (CFA) with this data it's a good idea to look at histograms, box plots,stem and leaf plots, and bivariate relationships between the manifest variables using a scatter plot to get a sense of potentially influential/outlying observations. To make a scatter plot matrix we can use the `pairs()` function. Since we have 11 variables, we would have 121 subgraphs! So, let's look at the scatter plot matrix by the latent construct they are theoretically suppose to be manifestations of. 

We'll start by making box plots.
```{r, fig.height=7}
wiscr.sub <- wiscr[,-(1:2)]
par(mfrow = c(3, 4),
    mar = c(1, 2, 2, 1) + .1)
for(i in 1:11){
  boxplot(wiscr.sub[,i], main = paste(names(wiscr.sub))[i])
}
```
For most variables, there are a few observations that are outliers. These outliers could be problematic moving forward and could influence our findings and therefore the stability of our estimated parameters. Some of these variables are also quite skewed as indicated by the asymmetry of the boxes. However, what is a univariate outlier or influential point may not be a multivariate outlier or influential point. The multivariate influential plot, shown below, might be a bit more insightful for assessing outliers.

Next lets look at the bivariate scatter plots starting with the manifest variables for the verbal factor.

```{r}
# this function will write the correlations in the upper diagonal
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- cor(x, y)
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

dev.off() # this resets par()
verb.scat <- subset(wiscr.sub, select = c("info", "comp", "arith", "simil", "digit", "vocab"))
pairs(verb.scat, upper.panel = panel.cor)
```

In general, we see moderately strong, positive relationships between all the subtests with the possible exception of `digit`, which appears to be less strongly correlated with the other subtests. 

Next we examine the manifest variables for the performance factor.
```{r}
perf.scat <- subset(wiscr.sub, select = !(names(wiscr.sub) %in% names(verb.scat)))
pairs(perf.scat, upper.panel = panel.cor)
```

Again, we see moderately strong positive relationships between all the subtests with the exception of `coding`.

To investigate influence, we can also look at Mahalanobis distance using the `faoutlier` package. This measures the distance an observation is from the centriod (a multidimensional mean).

```{r, message = FALSE}
# Install devtools
install.packages('devtools')
library(devtools)

# Install faoutlier from GitHub
install_github('philchalmers/faoutlier')

wiscr.mah <- faoutlier::robustMD(wiscr)
plot(wiscr.mah)
which.max(wiscr.mah$mah)
```

There are a few observations near 40 that look influential. A sensible thing to do would be to consider the robustness of our CFA findings or your covariance matrix to omitting these observations (i.e., as a sensitivity analysis). I leave that as an exercise for you. If you had values that were very far from the pack and a small sample size, then it would be even more imperative to do this. However, later we look at Cook's distance and those are values that are influential based on your actual model and might be more important to investigate. 

For now, let's move on to performing the actual CFA.

### CFA using `lavaan`

To perform the actual confirmatory factor analysis in R we'll use `lavaan` package and we'll use the `semPlot` package to create path diagrams feeding the function from our `lavaan` objects.

```{r, message = FALSE}
# Install the package, if you haven't yet.
# install.packages("semPlot")

# Load the lavaan and semPlot packages
library("semPlot")
library("lavaan")
```

As a reminder, in `lavaan`, the following syntax is used

  + "=~" means is manifested by. This is how you define your latent variable.
  + "~~" is how you define a covariance or variance.
  + "~" is how you define a path for path analysis or SEM.
  + "+" is how you string together variables (like with `lm()`)

Since we know which subtests were suppose to measure which component of intelligence (i.e., Verbal or Performance), we will define our model to reflect this. 

```{r}
# Step 1: define the model using lavaan syntax
# quick note: the =~ means is factor X is a manifestation of A + B + C
iq.cfa <- '
  verb =~ info + comp + arith + simil + digit + vocab
  perf =~ pictcomp + parang + block + object + coding
'
```

Now let's fit the model with the `cfa()` function. We could also use the `sem()` function, however the author of the package warns.

\begin{quote}
The function sem() is very similar to the function cfa(). In fact, the two functions are currently almost identical, but this may change in the future.
\end{quote}

Since we're doing a CFA, let's use the `cfa()` function to fit the model and save it to an object called `fit`. Also, note because we use the package `faoutlier`, it loaded the package called `sem`, which annoyingly has a function also called `cfa()`. So, you might need to unload the `sem` package or call `cfa()` via `lavaan::cfa()` if you get an error.

```{r}
fit <- cfa(iq.cfa, data = wiscr)
```

Before we look at the output, let's see what the path diagram looks like.
```{r}
semPaths(fit)
```

Let's also inspect the model to verify that `lavaan` actually fit the model we wanted.

```{r}
inspect(fit)
```

In the `$lamba$` matrix, we see which factor loadings (pattern coefficients) are being estimated. This is indicated by a non-zero value. So, you'll note that `info` and `pictcomp` are both zero. This is so that the model can be identified (i.e., we are using these to define a scale for our factor).

Next, is `$theta`, the variance-covariance matrix of our manifest variables, this refers to the residual variances for our manifest variables, i.e., the unique or specific variance. This is variability that is unexplained by our factor models. You'll see that all the manifest variables have this estimated and we have no covariances estimated. This means that residuals are uncorrelated (i.e., locally-independent).

Finally, you can `$psi` which is the variance-covariance matrix of our factors. You'll note that the elements of this matrix (the variances and covariance) are being estimated (i.e., we are assuming verbal and performance intelligence are correlated). Unlike LISREL notation, regardless of whether a latent variable is exogenous or endogenous latent variables (i.e., it will include disturbances) they will be in the $psi matrix. In LISREL notation, psi contains only endogenous latent variables disturbances and the phi matrix contains exogenous latent variables variances/covariances.

To obtain the default summary output, we'll call the summary function over our `fit` object.

```{r}
summary(fit)
```

The output is meant to be similar to `Mplus`. However, the default output is a bit more terse. The "Minimum Function Test Statistic", is the chi-square statistic for the current model and is essentially is a measure of deviance between your model-implied variance/covariance matrix and your observed variance/covariance matrix (what would be a fully-saturated model). Because we reject the null hypothesis this means that our reduced model-implied covariance matrix doesn't fit as well as the fully-saturated model and suggests a poor fitting model. 

After this are the parameter estimates. The major thing to note here is that `coding` doesn't appear to be related to performance IQ. 

Unlike `Mplus`, `lavaan` does not estimate intercepts by default. If you are interested in this, you need to specify `meanstructure = TRUE` during the fitting routine. This will not change the overall fit of the model as `lavaan` will plug in the mean of your observed variables as the estimates (i.e., it doesn't change the degrees of freedom). Let's look at the model.

```{r}
fit <- cfa(iq.cfa, data = wiscr, meanstructure = TRUE)
inspect(fit)
```

The big difference is the presence of the `$nu`, which are the intercepts for the observed variables (i.e., their means) and `$alpha`, which are the means for verbal and performance. These are fixed to 0 by default. This can be confusing because while these parameters are being estimated, they are not really being estimated because we're just using the means for their estimates.  

```{r}
fit <- cfa(iq.cfa, data = wiscr, meanstructure = TRUE)
summary(fit)
```

The output starts the same as previously but now we got an additional "Model test baseline model". What is this model? We talked about it earlier in the semester.

Next are some fit indices, CFI and TLI, we wants these to be large, preferably at least .95. This is followed by the log-likelihoods of our current model and the unrestricted model (a fully saturated model). Followed by the number of free parameters, we can get this from inspect too, and the AIC and BIC, which are calculated based on the log-likelihood and df. Finally, the RMSEA and SRMR are presented and we want these to be small. Preferably, smaller than .05 but less than .08 is at least okay. 

Please note, that some SEM methodologists don't think we should look at these statistics at all and instead should only interpret the first chi-square test. If we reject this statistic, then we should stop. We'll talk more about fit a little later in the semester.

If you want to request the standard SEM fit indices, you can obtain them either directly:

```{r}
fitmeasures(fit, c("tli", "cfi", "rmsea.ci.lower", "rmsea", "rmsea.ci.upper", "srmr", "aic", "bic"))
```

### Residuals

We can and should inspect the residual covariance matrix to assess local fit. We will examine the residual correlation matrix ($cov) and the standardized residuals ($cov.z). We are looking for correlations that are large than .1 (in absolute value) and for the standardized residuals if a value is greater than 1.96 (in absolute value) this indicates a statistically significant lack of fit and they can be interpreted as z-tests.

```{r}
lavResiduals(fit)
```

Based on the residual correlation matrix, it looks the covariance between pictcomp and comp is being overestimated by the model as are pictcomp and simil, parang and simil, and coding and vocab. The covariances between pictcomp and coding is being underestimated as is object and arith. These values imply misfit for at least part of the covariance matrix.

Based on the standardized residuals, comp and info, comp and pictcomp, arith and object, simil and pictcomp, digit and coding, and pictcomp and coding are not being adequately explained by the model (z > 1.96). 

This mismatch can explain the poor fit seen above.

## Interpreting parameters

To obtain the standardized solution and get more fit indices,

```{r}
fit <- cfa(iq.cfa, data = wiscr)
summary(fit, standardized = TRUE, fit.measures = TRUE)
```

Because all of our latent variables are continuous, we will use `Std.all` for all the variables (which standardizes both the observed and latent variables). The interpretation is the same as  beta weights. For example, for info, 0.423 means for a 1 standard deviation in verbal, the expected increase in info is 0.423 standard deviations. If we square these, that is the proportion of the manifest variable explained by the factor (i.e r-squared) because we have a simple solution.  If we had dichotomous predictor variables, we would use `Std.lv`, which represents the expected change in standard deviations in the outcome variable as the dichotomous variable goes from 0 to 1. 

We can have lavaan give us the r-squared directly, which can also be calculated by just doing 1 - the standardized unique variance.

```{r}
summary(fit, rsquare = T)
inspect(fit, "rsquare")
```

So what is the "Model test baseline model" again?

```{r}
iq.baseline <- '
  info ~~ info
  comp ~~ comp
  arith ~~ arith
  simil ~~ simil
  digit ~~ digit
  vocab ~~ vocab
  pictcomp ~~ pictcomp
  parang ~~ parang
  block ~~ block
  object ~~ object
  coding ~~ coding
'
fit.base <- cfa(model = iq.baseline, data = wiscr)
summary(fit.base, fit.measures = T)
```

It's the model that estimates only variances and no other parameters. So the df of 55, is 66 (number of unique elements) - 11 (number of variances). This model is suppose to be the most parsimonious plausible model (a model with no association). 

Returning to our model, Let's fit the parameter for coding to 0 in our the model and refit the model:

```{r}
iq.cfa.nocode <- '
  verb =~ info + comp + arith + simil + digit + vocab
  perf =~ pictcomp + parang + block + object + 0*coding
'
fit.nocode <- cfa(iq.cfa.nocode, wiscr)
summary(fit.nocode, fit.measures = TRUE, standardized = TRUE)
```

Let's compare the model with coding and without coding

```{r}
nocode.stats <- fitmeasures(fit.nocode, fit.measures = c("rmsea", "tli", "cfi"))
code.stats <- fitmeasures(fit, fit.measures = c("rmsea", "tli", "cfi"))   
rbind(nocode.stats, code.stats)
anova(fit.nocode, fit) # chi-square test of difference. 
```

We could consider dropping coding but dropping coding changes the intepretation of the construct. It is a more parsimonious model but if it's a theoretical important measure to our construct, it should probably remain.

Next, let's examine the modification indices. Modification indices can be used to improve the fit of your model and look at the improvement in model fit (chi-square) if that path is freed. Our present model is nested within this.

```{r}
iq.cfa.nocode <- '
  verb =~ info + comp + arith + simil + digit + vocab
  perf =~ pictcomp + parang + block + object
'
fit.nocode <- cfa(iq.cfa.nocode, wiscr)
modindices(fit.nocode, sort. = TRUE)
```

The first column is the name of the path to free, followed by the expected change in chi-square, the expected parameter change (i.e., the estimated parameter) and then standardized versions of this: standardizing just the latent variable, standardizing all variables, standardizing all but exogenous variables in the model.  

The biggest change in model fit would be allowing comp to load onto perf. The change in chi-square should be 9.853.
```{r}
iq.cfa.emp <- '
  verb =~ info + comp + arith + simil + digit + vocab
  perf =~ pictcomp + parang + block + object + comp 
'

fit.emp <- cfa(iq.cfa.emp, wiscr)
summary(fit.emp, fit.measures = TRUE, standardized = TRUE)
```

Now we see that are fit measures are very good and fail to reject the null hypothesis for the chi-square test. However, does it make sense to load comp on perf? That's a validity question and has ramifications for generalizability as well. 

Now let's plot our best fitting model with the standardized parameters.

```{r}
semPaths(fit.emp, what = "stand", fade = F)
```

Finally, let's see if there are any influential points using Cook's distance.

```{r}
cfa.cooks <- faoutlier::gCD(data = wiscr, mod = iq.cfa.emp)
plot(cfa.cooks)
```

There are definitely a few observations which are quite far from the rest. Let's see if the results change if we drop the value with the largest Cook's distance.

```{r}
which(cfa.cooks$gCD )
cfa.cooks$gCD[76]
fit.emp.no76 <- cfa(iq.cfa.emp, wiscr[-c(76),])
summary(fit.emp.no76)
```

What do you think? Did they change?
